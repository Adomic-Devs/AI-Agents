{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Building Financial AI Agents Session ðŸ˜€\n",
    "###Hosted by: DataCamp\n",
    "###Led by: Jayeeta Putatunda\n",
    "###Date: Feb 11, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Goal of the Session:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Build AI Agents to handle various financial tasks\n",
    "\n",
    "2. Four agents:\n",
    "  - Tool Calling - Websearch\n",
    "  - For RAG based query\n",
    "  - Deep Research Stock Analysis\n",
    "  - Evaluation (LLM-as-a-judge) Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Open-Source Tech Stack:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Fast OS LLM Inference [Groq]: https://groq.com/\n",
    "- Agentic Framework [Agno]: https://www.agno.com/\n",
    "- Vector Database [PgVector]: https://pypi.org/project/pgvector/\n",
    "- Embeddings [Sentence-transformers]: https://huggingface.co/sentence-transformers\n",
    "- Containerization [Udocker for Colab]: https://github.com/drengskapur/docker-in-colab\n",
    "- Websearch [DuckDuckGo]: https://github.com/duckduckgo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Samin Chandeepa/AppData/Local/Microsoft/WindowsApps/python3.12.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip install groq yfinance agno\n",
    "!pip install groq duckduckgo-search newspaper4k lxml_html_clean agno\n",
    "!pip install -U sqlalchemy 'psycopg[binary]' pgvector pypdf agno\n",
    "!pip install udocker\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
    "os.environ['PHI_API_KEY'] = userdata.get('PHI_API_KEY') #AGNO KEY (I had this key before the rebrand)\n",
    "\n",
    "print(\"API keys have been set!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF you want to check if the keys are indeed set\n",
    "\n",
    "# import os\n",
    "print(\"Groq API key set:\" if os.getenv(\"PHI_API_KEY\") else \"Groq API key not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Agent 1:\n",
    "##Functional Tool Calling Capability: Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from agno.agent import Agent\n",
    "from agno.models.groq import Groq\n",
    "from agno.tools.duckduckgo import DuckDuckGoTools\n",
    "from agno.tools.newspaper4k import Newspaper4kTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the research agent with advanced journalistic capabilities\n",
    "research_agent = Agent(\n",
    "    model=Groq(id=\"llama3-70b-8192\"),\n",
    "    tools=[DuckDuckGoTools(), Newspaper4kTools()],\n",
    "    description=dedent(\"\"\"\\\n",
    "        You are an elite research analyst in the financial services domain.\n",
    "        Your expertise encompasses:\n",
    "\n",
    "        - Deep investigative financial research and analysis\n",
    "        - fact-checking and source verification\n",
    "        - Data-driven reporting and visualization\n",
    "        - Expert interview synthesis\n",
    "        - Trend analysis and future predictions\n",
    "        - Complex topic simplification\n",
    "        - Ethical practices\n",
    "        - Balanced perspective presentation\n",
    "        - Global context integration\\\n",
    "    \"\"\"),\n",
    "    instructions=dedent(\"\"\"\\\n",
    "        1. Research Phase\n",
    "           - Search for 5 authoritative sources on the topic\n",
    "           - Prioritize recent publications and expert opinions\n",
    "           - Identify key stakeholders and perspectives\n",
    "\n",
    "        2. Analysis Phase\n",
    "           - Extract and verify critical information\n",
    "           - Cross-reference facts across multiple sources\n",
    "           - Identify emerging patterns and trends\n",
    "           - Evaluate conflicting viewpoints\n",
    "\n",
    "        3. Writing Phase\n",
    "           - Craft an attention-grabbing headline\n",
    "           - Structure content in Financial Report style\n",
    "           - Include relevant quotes and statistics\n",
    "           - Maintain objectivity and balance\n",
    "           - Explain complex concepts clearly\n",
    "\n",
    "        4. Quality Control\n",
    "           - Verify all facts and attributions\n",
    "           - Ensure narrative flow and readability\n",
    "           - Add context where necessary\n",
    "           - Include future implications\n",
    "    \"\"\"),\n",
    "    expected_output=dedent(\"\"\"\\\n",
    "        # {Compelling Headline}\n",
    "\n",
    "        ## Executive Summary\n",
    "        {Concise overview of key findings and significance}\n",
    "\n",
    "        ## Background & Context\n",
    "        {Historical context and importance}\n",
    "        {Current landscape overview}\n",
    "\n",
    "        ## Key Findings\n",
    "        {Main discoveries and analysis}\n",
    "        {Expert insights and quotes}\n",
    "        {Statistical evidence}\n",
    "\n",
    "        ## Impact Analysis\n",
    "        {Current implications}\n",
    "        {Stakeholder perspectives}\n",
    "        {Industry/societal effects}\n",
    "\n",
    "        ## Future Outlook\n",
    "        {Emerging trends}\n",
    "        {Expert predictions}\n",
    "        {Potential challenges and opportunities}\n",
    "\n",
    "        ## Expert Insights\n",
    "        {Notable quotes and analysis from industry leaders}\n",
    "        {Contrasting viewpoints}\n",
    "\n",
    "        ## Sources & Methodology\n",
    "        {List of primary sources with key contributions}\n",
    "        {Research methodology overview}\n",
    "\n",
    "        ---\n",
    "        Research conducted by Financial Agent\n",
    "        Credit Rating Style Report\n",
    "        Published: {current_date}\n",
    "        Last Updated: {current_time}\\\n",
    "    \"\"\"),\n",
    "    markdown=True,\n",
    "    show_tool_calls=True,\n",
    "    add_datetime_to_instructions=True,\n",
    ")\n",
    "\n",
    "# User Prompt 1\n",
    "research_agent.print_response(\"Analyze the current state and future implications \\\n",
    "                              of artificial intelligence in Finance\",stream=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Prompt 2\n",
    "research_agent.print_response(\"Applications of Gen AI in Financial Services\",stream=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Prompt 3\n",
    "research_agent.print_response(\"AI agentsin Financial Services\",stream=True,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 2:\n",
    "## Knowledge base Query Capability: RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using OS Vector Databases, we need to use docker to launch the app and initialize the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Drengskapur\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "# @title {display-mode:\"form\"}\n",
    "# @markdown <br/><br/><center><img src=\"https://cdn.jsdelivr.net/gh/drengskapur/docker-in-colab/assets/docker.svg\" height=\"150\"><img src=\"https://cdn.jsdelivr.net/gh/drengskapur/docker-in-colab/assets/colab.svg\" height=\"150\"></center><br/>\n",
    "# @markdown <center><h1>Docker in Colab</h1></center><center>github.com/drengskapur/docker-in-colab<br/><br/><br/><b>udocker(\"run hello-world\")</b></center><br/>\n",
    "def udocker_init():\n",
    "    import os\n",
    "    if not os.path.exists(\"/home/user\"):\n",
    "        !pip install udocker > /dev/null\n",
    "        !udocker --allow-root install > /dev/null\n",
    "        !useradd -m user > /dev/null\n",
    "    print(f'Docker-in-Colab 1.1.0\\n')\n",
    "    print(f'Usage:     udocker(\"--help\")')\n",
    "    print(f'Examples:  https://github.com/indigo-dc/udocker?tab=readme-ov-file#examples')\n",
    "\n",
    "    def execute(command: str):\n",
    "        user_prompt = \"\\033[1;32muser@pc\\033[0m\"\n",
    "        print(f\"{user_prompt}$ udocker {command}\")\n",
    "        !su - user -c \"udocker $command\"\n",
    "\n",
    "    return execute\n",
    "\n",
    "udocker = udocker_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to initialize the PGVector. It's simpler if you are running from your local terminal but to make it work in the Colab, we need to run the below steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First install udocker\n",
    "!udocker --allow-root install\n",
    "\n",
    "# 2. Kill existing processes and clean up\n",
    "!pkill -9 -f postgres\n",
    "!rm -rf /content/pgdata\n",
    "!udocker --allow-root rm pgvector\n",
    "!rm -f postgres.log\n",
    "\n",
    "# 3. Create fresh directory\n",
    "!mkdir -p /content/pgdata\n",
    "!chmod -R 777 /content/pgdata\n",
    "\n",
    "# 4. Pull and create container with correct image path\n",
    "!udocker --allow-root pull ankane/pgvector\n",
    "!udocker --allow-root create --name=pgvector ankane/pgvector\n",
    "\n",
    "# 5. Run the container\n",
    "!nohup udocker --allow-root run \\\n",
    "    --env=\"POSTGRES_DB=ai\" \\\n",
    "    --env=\"POSTGRES_USER=ai\" \\\n",
    "    --env=\"POSTGRES_PASSWORD=ai\" \\\n",
    "    --env=\"PGDATA=/var/lib/postgresql/data/pgdata\" \\\n",
    "    --volume=\"/content/pgdata:/var/lib/postgresql/data\" \\\n",
    "    --publish=\"5532:5432\" \\\n",
    "    pgvector > postgres.log 2>&1 &\n",
    "\n",
    "# 6. Connection testing\n",
    "import time\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import OperationalError\n",
    "\n",
    "def test_db_connection(max_retries=5, wait_time=10):\n",
    "    db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"\\nConnection attempt {attempt + 1}/{max_retries}\")\n",
    "            engine = create_engine(db_url)\n",
    "            with engine.connect() as connection:\n",
    "                result = connection.execute(text(\"SELECT version();\"))\n",
    "                version = result.fetchone()[0]\n",
    "                print(\"âœ… Successfully connected to PostgreSQL!\")\n",
    "                print(f\"Server Version: {version}\")\n",
    "\n",
    "                # Test vector extension\n",
    "                connection.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector;\"))\n",
    "                print(\"âœ… Vector extension ready!\")\n",
    "                return engine\n",
    "        except OperationalError as e:\n",
    "            print(f\"Attempt {attempt + 1} failed, waiting {wait_time} seconds...\")\n",
    "            print(\"\\nChecking postgres status:\")\n",
    "            !ps aux | grep postgres\n",
    "            print(\"\\nLatest logs:\")\n",
    "            !tail -n 20 postgres.log\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "    return None\n",
    "\n",
    "# 7. Apply 30 secs sleep time to wait for DB to finish set up before testing for connection\n",
    "print(\"Waiting for database to initialize...\")\n",
    "time.sleep(30)\n",
    "\n",
    "engine = test_db_connection()\n",
    "\n",
    "if engine:\n",
    "    print(\"\\nâœ… Database is ready for RAG Agent initialization!\")\n",
    "else:\n",
    "    print(\"\\nâŒ Database connection failed. Please check the logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class **Documemt QA**:\n",
    "- Initialize a OS sentence-transformer embedding model\n",
    "- Access the Vector DB path to load the embeddings of the PDF URL passed by user\n",
    "- Initialize database, ready to run QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Tuple\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install phidata\n",
    "from phi.knowledge.pdf import PDFUrlKnowledgeBase\n",
    "from phi.vectordb.pgvector import PgVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentQA:\n",
    "    def __init__(self):\n",
    "        # Initialize embedder\n",
    "        self.embedder = self._create_embedder()\n",
    "        # Initialize Groq model\n",
    "        self.chat_model = Groq(id=\"llama3-8b-8192\")\n",
    "        # Database URL\n",
    "        self.db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n",
    "        self.current_knowledge_base = None\n",
    "        self.agent = None\n",
    "        self.dimensions = 384\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.model.encode(query).tolist()\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "    def _create_embedder(self):\n",
    "        \"\"\"Create the embedding model\"\"\"\n",
    "        class EmbeddingModel:\n",
    "            def __init__(self):\n",
    "                self.model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "                self.dimensions = 384\n",
    "\n",
    "            def get_embedding_and_usage(self, text: Union[str, List[str]]) -> Tuple[Union[List[List[float]], List[float]], dict]:\n",
    "                if isinstance(text, str):\n",
    "                    embedding = self.model.encode(text)\n",
    "                    embedding_list = embedding.tolist()\n",
    "                    usage = {\"prompt_tokens\": len(text.split()), \"total_tokens\": len(text.split())}\n",
    "                    return embedding_list, usage\n",
    "                else:\n",
    "                    embeddings = self.model.encode(text)\n",
    "                    embedding_list = embeddings.tolist()\n",
    "                    total_tokens = sum(len(t.split()) for t in text)\n",
    "                    usage = {\"prompt_tokens\": total_tokens, \"total_tokens\": total_tokens}\n",
    "                    return embedding_list, usage\n",
    "\n",
    "            def get_embedding(self, text: Union[str, List[str]]) -> Union[List[float], List[List[float]]]:\n",
    "                if isinstance(text, str):\n",
    "                    return self.model.encode(text).tolist()\n",
    "                return self.model.encode(text).tolist()\n",
    "\n",
    "        print(\"âœ… Embedding model(sentence-transformers/paraphrase-MiniLM-L6-v2) initialized successfully!\")\n",
    "        return EmbeddingModel()\n",
    "\n",
    "    def load_pdf_url(self, url: str, table_name: str = \"documents\"):\n",
    "        \"\"\"Load a PDF from a URL\"\"\"\n",
    "        try:\n",
    "            # Create PDF URL knowledge base\n",
    "            self.current_knowledge_base = PDFUrlKnowledgeBase(\n",
    "                urls=[url],\n",
    "                vector_db=PgVector(\n",
    "                    table_name=table_name,\n",
    "                    db_url=self.db_url,\n",
    "                    embedder=self.embedder\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Initialize the Agent\n",
    "            self.agent = Agent(\n",
    "                knowledge=self.current_knowledge_base,\n",
    "                search_knowledge=True,\n",
    "                model=self.chat_model\n",
    "            )\n",
    "\n",
    "            # Load knowledge base\n",
    "            print(\"Loading knowledge base...\")\n",
    "            self.current_knowledge_base.load(recreate=True)\n",
    "            print(\"âœ… Knowledge base loaded successfully!\")\n",
    "\n",
    "            # Show sample content\n",
    "            self.show_sample_content()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading PDF: {e}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "    def show_sample_content(self, num_samples: int = 5):\n",
    "        \"\"\"Show sample content from the knowledge base\"\"\"\n",
    "        try:\n",
    "            if not self.current_knowledge_base:\n",
    "                print(\"No knowledge base loaded!\")\n",
    "                return\n",
    "\n",
    "            docs = self.current_knowledge_base.search(\"\")\n",
    "            print(\"\\nSample documents in knowledge base:\")\n",
    "            print(\"-\" * 50)\n",
    "            for i, doc in enumerate(docs[:num_samples], 1):\n",
    "                print(f\"\\nDocument {i}:\")\n",
    "                if hasattr(doc, 'content'):\n",
    "                    print(doc.content[:200] + \"...\" if len(doc.content) > 200 else doc.content)\n",
    "                elif hasattr(doc, 'text'):\n",
    "                    print(doc.text[:200] + \"...\" if len(doc.text) > 200 else doc.text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error showing samples: {e}\")\n",
    "\n",
    "    def ask(self, question: str):\n",
    "        \"\"\"Ask a question about the loaded document\"\"\"\n",
    "        if not self.current_knowledge_base or not self.agent:\n",
    "            print(\"Please load a document first!\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nQ: {question}\")\n",
    "        try:\n",
    "            # Get relevant documents\n",
    "            relevant_docs = self.current_knowledge_base.search(question)\n",
    "            print(\"\\nRelevant documents found:\", len(relevant_docs) if relevant_docs else 0)\n",
    "\n",
    "            # Build context from relevant documents\n",
    "            context = \"\\n\".join([doc.content if hasattr(doc, 'content') else doc.text\n",
    "                               for doc in relevant_docs])\n",
    "\n",
    "            # Create a prompt that includes the context\n",
    "            full_prompt = f\"\"\"Based on the following content:{context}\n",
    "            Question: {question}\n",
    "            Please provide a detailed answer based ONLY on the information provided above.\"\"\"\n",
    "\n",
    "            # Get response with context\n",
    "            response = self.agent.run(full_prompt)\n",
    "            print(f\"\\nA: {response.content}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_qa = DocumentQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_qa.load_pdf_url(\"https://www.apple.com/environment/pdf/Apple_Environmental_Progress_Report_2024.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_qa.ask(\"Key points in this report? Give in 5 bullets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_qa.ask(\"Executive Summary in 100 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 3:\n",
    "##Stock Market analysis\n",
    "\n",
    "1. Utilize yahoo finance to run comparative analysis using many\n",
    "2. Generate a small summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "from agno.agent import Agent\n",
    "from agno.models.openai import OpenAIChat\n",
    "from agno.tools.yfinance import YFinanceTools\n",
    "\n",
    "stock_agent = Agent(\n",
    "    model=Groq(id=\"llama3-70b-8192\"),\n",
    "    tools=[\n",
    "        YFinanceTools(\n",
    "            stock_price=True,\n",
    "            analyst_recommendations=True,\n",
    "            stock_fundamentals=True,\n",
    "            historical_prices=True,\n",
    "            company_info=True,\n",
    "            company_news=True,\n",
    "        )\n",
    "    ],\n",
    "    instructions=dedent(\"\"\"\\\n",
    "        You are a seasoned credit rating analyst with deep expertise in market analysis! ðŸ“Š\n",
    "\n",
    "        Follow these steps for comprehensive financial analysis:\n",
    "        1. Market Overview\n",
    "           - Latest stock price\n",
    "           - 52-week high and low\n",
    "        2. Financial Deep Dive\n",
    "           - Key metrics (P/E, Market Cap, EPS)\n",
    "        3. Market Context\n",
    "           - Industry trends and positioning\n",
    "           - Competitive analysis\n",
    "           - Market sentiment indicators\n",
    "\n",
    "        Your reporting style:\n",
    "        - Begin with an executive summary\n",
    "        - Use tables for data presentation\n",
    "        - Include clear section headers\n",
    "        - Highlight key insights with bullet points\n",
    "        - Compare metrics to industry averages\n",
    "        - Include technical term explanations\n",
    "        - End with a forward-looking analysis\n",
    "\n",
    "        Risk Disclosure:\n",
    "        - Always highlight potential risk factors\n",
    "        - Note market uncertainties\n",
    "        - Mention relevant regulatory concerns\n",
    "    \"\"\"),\n",
    "    add_datetime_to_instructions=True,\n",
    "    show_tool_calls=True,\n",
    "    markdown=True,\n",
    ")\n",
    "\n",
    "print(\"Stock Agent created. Ready to take user queries..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# User Query 1\n",
    "stock_agent.print_response(\n",
    "    \"What's the latest news and financial performance of Apple (AAPL)?\", stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Query 2: Semiconductor market analysis\n",
    "finance_agent.print_response(\n",
    "    dedent(\"\"\"\\\n",
    "    Analyze the semiconductor market performance focusing on:\n",
    "    - NVIDIA (NVDA)\n",
    "    - AMD (AMD)\n",
    "    - Intel (INTC)\n",
    "    - Taiwan Semiconductor (TSM)\n",
    "    Compare their market positions, growth metrics, and future outlook in terms of AI growth.\"\"\"),\n",
    "    stream=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Query 3: Competitive analysis\n",
    "\n",
    "finance_agent.print_response(\"How is Microsoft performing in the age of AI?\", stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 4\n",
    "\n",
    "## Evaluation: LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from agno.agent import Agent\n",
    "from agno.models.groq import Groq\n",
    "\n",
    "class RAGEvaluator:\n",
    "    def __init__(self):\n",
    "        self.evaluator = self._initialize_evaluator()\n",
    "\n",
    "    def _initialize_evaluator(self):\n",
    "        return Agent(\n",
    "            model=Groq(id=\"llama-3.1-8b-instant\"),  # Using different Llama model\n",
    "            description=dedent(\"\"\"\\\n",
    "                You are an expert RAG system evaluator with deep expertise in:\n",
    "                - Information retrieval quality assessment\n",
    "                - Response accuracy evaluation\n",
    "                - Source attribution verification\n",
    "                - Context relevance analysis\n",
    "                - Natural language generation evaluation\n",
    "            \"\"\"),\n",
    "            instructions=dedent(\"\"\"\\\n",
    "                Evaluate the RAG system output based on these key metrics:\n",
    "\n",
    "                1. Faithfulness (1-5):\n",
    "                   - How accurately does the response reflect the source documents?\n",
    "                   - Are there any hallucinations or incorrect statements?\n",
    "                   - Does it maintain factual consistency?\n",
    "\n",
    "                2. Context Relevance (1-5):\n",
    "                   - Are the retrieved passages relevant to the query?\n",
    "                   - Is important context missing?\n",
    "                   - Is irrelevant information included?\n",
    "\n",
    "                3. Answer Completeness (1-5):\n",
    "                   - Does the response fully address the query?\n",
    "                   - Are all key aspects covered?\n",
    "                   - Is the level of detail appropriate?\n",
    "\n",
    "                4. Source Attribution (1-5):\n",
    "                   - Are sources properly cited?\n",
    "                   - Is it clear which information comes from where?\n",
    "                   - Can claims be traced back to sources?\n",
    "\n",
    "                5. Response Coherence (1-5):\n",
    "                   - Is the response well-structured?\n",
    "                   - Does it flow logically?\n",
    "                   - Is it easy to understand?\n",
    "\n",
    "                Provide specific examples and explanations for each score.\n",
    "            \"\"\"),\n",
    "            expected_output=dedent(\"\"\"\\\n",
    "                # RAG Evaluation Report\n",
    "\n",
    "                ## Overview\n",
    "                Query: {query}\n",
    "                Response Length: {n_chars} characters\n",
    "\n",
    "                ## Metric Scores\n",
    "\n",
    "                ### Faithfulness: {score}/5\n",
    "                - Justification:\n",
    "                - Examples:\n",
    "                - Areas for Improvement:\n",
    "\n",
    "                ### Context Relevance: {score}/5\n",
    "                - Justification:\n",
    "                - Examples:\n",
    "                - Areas for Improvement:\n",
    "\n",
    "                ### Answer Completeness: {score}/5\n",
    "                - Justification:\n",
    "                - Examples:\n",
    "                - Areas for Improvement:\n",
    "\n",
    "                ### Source Attribution: {score}/5\n",
    "                - Justification:\n",
    "                - Examples:\n",
    "                - Areas for Improvement:\n",
    "\n",
    "                ### Response Coherence: {score}/5\n",
    "                - Justification:\n",
    "                - Examples:\n",
    "                - Areas for Improvement:\n",
    "\n",
    "                ## Overall Score: {total}/25\n",
    "\n",
    "                ## Key Recommendations\n",
    "                1. {rec1}\n",
    "                2. {rec2}\n",
    "                3. {rec3}\n",
    "\n",
    "                ## Summary\n",
    "                {final_assessment}\n",
    "            \"\"\"),\n",
    "            markdown=True,\n",
    "        )\n",
    "\n",
    "    def evaluate(self, query: str, response: str, context: list, stream: bool = True):\n",
    "        \"\"\"\n",
    "        Evaluate a RAG system's response\n",
    "\n",
    "        Args:\n",
    "            query (str): Original user query\n",
    "            response (str): RAG system's response\n",
    "            context (list): Retrieved passages used for the response\n",
    "            stream (bool): Whether to stream the evaluation output\n",
    "        \"\"\"\n",
    "        evaluation_prompt = f\"\"\"\n",
    "        Please evaluate this RAG system output:\n",
    "\n",
    "        QUERY:\n",
    "        {query}\n",
    "\n",
    "        RETRIEVED CONTEXT:\n",
    "        {' '.join(context)}\n",
    "\n",
    "        RESPONSE:\n",
    "        {response}\n",
    "\n",
    "        Provide a detailed evaluation following the metrics and format specified.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.evaluator.print_response(evaluation_prompt, stream=stream)\n",
    "\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RAGEvaluator()\n",
    "print(\"LLM-as-a Judge Evaluator initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example evaluation. Rerun this to use actual financial RAG outputs\n",
    "\n",
    "query = \"What are the key features of transformer models?\"\n",
    "context = [\n",
    "    \"Transformer models use self-attention mechanisms to process input sequences.\",\n",
    "    \"Key features include parallel processing and handling of long-range dependencies.\"\n",
    "]\n",
    "response = \"Transformer models are characterized by their self-attention mechanism...\"\n",
    "\n",
    "# Run evaluation\n",
    "evaluator.evaluate(query, response, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Papers:\n",
    "1. The Rise and Potential of Large Language Model Based Agents: A Survey: https://arxiv.org/pdf/2309.07864\n",
    "2. Self-Reflection in LLM Agents: Effects on Problem-Solving Performance: https://arxiv.org/pdf/2405.06682v3\n",
    "3. Agent Laboratory: Using LLM Agents as Research Assistants: https://arxiv.org/pdf/2501.04227v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free Courses:\n",
    "1. By Hugging Face: https://huggingface.co/agents-course\n",
    "2. By Aishwarya Naresh Reganti:\n",
    "- https://github.com/aishwaryanr/awesome-generative-ai-guide/blob/main/resources/agents_101_guide.md\n",
    "- https://github.com/aishwaryanr/awesome-generative-ai-guide/blob/main/resources/agents_roadmap.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
